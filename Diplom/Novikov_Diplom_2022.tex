\documentclass[bachelor,subf
%,href
%,fixint=false
%,facsimile
%,times
%,draft
]{disser}
%%\documentclass[12pt,a4paper]{report}


\usepackage[
  a4paper, mag=1000, includefoot,
  left=2.5cm, right=1cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm,
]{geometry}
\usepackage[T2A]{fontenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi

\usepackage[pdftex,unicode,colorlinks,filecolor=blue,linkcolor=black,citecolor=blue,bookmarksopen,
pdfhighlight=/N]{hyperref}

%\onehalfspacing
%\chaptername{Гла}
\sloppy



%\def\thefootnote{\arabic{footnote})}
\def\ts{\thinspace}
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\D{\Delta}
\def\e{\varepsilon}
\def\f{\varphi}
\def\g{\gamma}
\def\G{\Gamma}
\def\o{\omega}
\def\O{\Omega}
\def\s{\sigma}
\def\vs{\varsigma}
\def\p{\partial}
\def\pr{\prime}
\def\vt{\vartheta}
\def\tl{\tilde}
\def\wtl{\widetilde}
\def\what{\widehat}
\def\l{\lambda}
\def\L{\Lambda}
\def\wtl{\widetilde}
\def\il{\int\limits}
\def\sul{\sum\limits}
\def\Rl{\rm Re}
\def\dsp{\displaystyle}
\def\ol{\overline}
\def\oe{\kappa}
\def\e{\varepsilon}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\t{\tilde}
\def\ln{\mathop{\mathrm{ln}\, }}
\def\ds{\displaystyle}
\def\dif{{\rm d}}
\def\what{\widehat}
\def\oLH{\omega_{\scriptscriptstyle \rm LH}}
\def\OLH{\Omega_{\scriptscriptstyle \rm LH}}
\def\oH{\omega_{\scriptscriptstyle \rm H}}
\def\OH{\Omega_{\scriptscriptstyle \rm H}}
\def\oUH{\omega_{\scriptscriptstyle \rm UH}}
\def\Op{\Omega_{\scriptscriptstyle \rm p}}
\def\op{\omega_{\scriptscriptstyle \rm p}}
\def\nue{\nu_{\scriptstyle \rm e}}
\def\nuei{\nu_{\scriptstyle \rm ei}}
\def\nuen{\nu_{\scriptstyle \rm en}}
\def\nuin{\nu_{\scriptstyle \rm in}}
\def\p{\scriptscriptstyle \rm p}

\def\vte{{\textit{v}}_{\scriptscriptstyle {T}_{\rm e}}}


\def\cP{\tilde P}
\def\cH{cB}
\def\EB{\bf E}
\def\Bb{\bf B}
\def\cHB{c{\bf B}}

\def\E{{\cal E}\kern-6.8pt{\cal E}\kern-6.8pt{\cal E}}
%\def\H{{\cal H}\kern-8.9pt{\cal H}\kern-9.1pt{\cal H}}
\def\H{{\cal H}\kern-9.9pt{\cal H}\kern-9.9pt{\cal H}}

\def\ve{{\boldmath {\mbox{$\varepsilon$}}}}

\def\r{\rho\kern-6.2pt\rho\kern-6.3pt\rho}
\def\ph{\phi\kern-6.7pt\phi\kern-6.7pt\phi}

\def\fo{\varphi\kern-8.2pt\varphi\kern-8.3pt\varphi}
\def\ol{\overline}

\def\i{\textrm{i}}
\def\d{\textrm{d}}

\renewcommand{\vec}[1]{{\bf#1}}
\renewcommand{\phi}{\varphi}
\renewcommand{\partial}{\d}

\renewcommand{\baselinestretch}{1.25}


\begin{document}
% Название организации
\begin{center}
\small{~\\ МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ\\ ~\\ Федеральное государственное автономное образовательное учреждение высшего образования\\ «Национальный исследовательский Нижегородский государственный университет
им. Н.\,И.\,Лобачевского» \\ ~\\
Радиофизический факультет \\ ~\\
Направление 02.03.02 «Фундаментальная информатика\\ и информационные технологии»\\
Профиль «Информационные системы и технологии»}
\end{center}
% Имя лица, допускающего к защите (зав. кафедрой)

\begin{center}
\normalsize{\textbf{ВЫПУСКНАЯ  КВАЛИФИКАЦИОННАЯ  РАБОТА}}
\end{center}

% Тема
\begin{center}
\normalsize{\textbf{Глубокие нейронные сети для распознавания формул на изображениях}}
\end{center}

\begin{flushleft}
\small{«К защите допущен»:

\hfill

Зав. кафедрой электродинамики\\ профессор, д.ф.-м.н.\hspace{10cm} Кудрин А.В.}
\end{flushleft}

\begin{flushleft}
\small{Научный руководитель,\\ ведущий инженер-исследователь,\\ к.ф.-м.н.\hspace{12,2cm} Еськин В.А.}
\end{flushleft}

\begin{flushleft}
\small{Рецензент,\\ доцент, к.ф.-м.н.\hspace{10,7cm} Дерябин М.С.}
\end{flushleft}

\begin{flushleft}
\small{Консультант по технике безопасности\\доцент, к.ф.-м.н.\hspace{10,7cm} Клемина А.В.}
\end{flushleft}

\begin{flushleft}
\small{Студент 4-го курса\hspace{10.23cm} Новиков Е.А.}
\end{flushleft}

\hfill

\hfill

% Город и год
\begin{center}
{Нижний Новгород\\ 2022}
\end{center}
\thispagestyle{empty}

% ----------------------------------------------------------------

% ----------------------------------------------------------------

% для замены в именах авторов использую регулярное выражение from (?<name>[A-Z?-?])\.\s(?<sname>[A-Z?-?])\. to  {${name}.\,${sname}.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\renewcommand{\chaptername}{}
%\renewcommand{\bibname}{Литература}
%\renewcommand{\figurename}{Рисунок}

%\setcounter{page}{6}
%\maketitle
% ----------------------------------------------------------------
\tableofcontents
% ----------------------------------------------------------------



\addcontentsline{toc}{section}{Введение}
\section*{Введение}
На данном этапе развития технологий в современном мире всё чаще появляется необходимость автоматизированного чтения текста с изображений, фото или видео. Под текстом может подразумеваться абсолютно всё: рукописный текст, формулы, требующие переноски из старых учебников и книг в сеть или быстрый перевод текста с бумажных носителей. В любом случае, такие технологии сильно облегчают жизнь людей.

Сейчас такую возможность нам предоставляют нейронные сети, которые, по сути, имитируют некоторые аспекты умственной деятельности человека, так как нейронная сеть – это модель, математически созданная на основе биологических нейронных сетей и их функционирования. Первая попытка создания нейронной сети принадлежит Уоррену Мак-Каллоку и Уолтеру Питтсу, которые формируют понятие нейронной сети [1]. А через несколько лет Дональд Хебб предлагает первый алгоритм обучения.

Интерес к нейронным сетям обусловлен их успешным применением в самых разных областях – медицина, бизнес, геология, физика. Их практикуют везде, где нужно находить решения задач управления, классификации или прогнозирования. Так Бернард Уидроу и его студент Хофф создали Адалин, использовавшийся для задач предсказания и адаптивного управления [2]. В 2007 году Джеффри Хитоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Для этого была использована ограниченная машина Больцмана[3]. Для обучения должно использоваться большое количество образов, которые могут быть распознанны. После обучения на выходе имеется быстро работающая программа с возможностью решения конкретных задач.

Целью работы является обучение нейросетей для распознования формул с изображений, одна из которых основанна на GRU ячейке, а вторая на модели ''Трансформер'', с последующим сравнением их результатов.

Актуальность данной работы состоит в практической необходимости распознования формул с изображений и относительно низким качеством распознования современными системами.

Данная работа состоит из двух глав. В первой главе рассматривается эксперимент с распознаванием формул на изображениях глубокой нейронной сетью, основанной на GRU ячейке. Во второй исследуется распознавания формул на изображениях глубокой нейронной сетью, основанной на модели "Трансформер".



\indent


\chapter{Распознавания формул на изображениях глубокой нейронной сетью, основанной на GRU ячейке}
%\addcontentsline{toc}{chapter}{Глава I Рассеяние плоской электромагнитной волны на миелинизированном аксоне}
\section{Описание набора данных и подготовки набора данных}

В качестве набора данных используются изображения с формулами, взятые с гарвардского проекта[4]. Набор состоит из двух пакетов "generateset" и ''hugedataset''. Первый представляет собой изображения с формулами в количестве 1700 штук, второй 104000 изображений. "generatedset" будет использоваться только для первой итерации(задания) каждой нейронной сети, для последующих итераций будет использован ''hugedataset''.  Процесс обучения состоит из 11 итераций. Для каждой из них устанавливаются свои критерии обучения. Изначально мы имеем изображения формул на листе формата A4 (\ref{fig:image1.1}).

\begin{figure}[H]
\center{\includegraphics[width=7.97cm]{h1.jpg}}
\includegraphics[width=4cm]{h11.jpg}
\caption{Первоначальный вид необработанного изображения с формулой.}
\label{fig:image1.1}
\end{figure}

Для оптимизации работы нейронной сети, предварительно, эти изображения обрезаются (\ref{fig:image1.2}). 

\begin{figure}[H]
\center{\includegraphics[width=7.97cm]{h2.jpg}}
\includegraphics[width=4cm]{h22.jpg}
\caption{Вид обрезанного изображения}
\label{fig:image1.2}
\end{figure}

После чего, формулы нормализуются. Создаётся отдельный файл, где каждая из формул прописана в печатном виде и имеет свой номер, расставляются все пробелы, прописываются названия греческих букв, вместо их обозначения. Для наибольшего успеха обучения, из пакета данных формулы исключаются те, что имеют большое количество грамматических ошибок.

\section{Описание модели}

\begin{figure}[H]
\center{\includegraphics[width=17cm]{gru.jpg}}
\caption{Схема нейронной сети, использованной в обучении.}
\label{fig:image1.3}
\end{figure}

Данная сеть (\ref{fig:image1.3}) представляет собой свёрточную нейронную сеть с несколькими стандартными нейронными компонентами из области зрения и обработки естественного языка. Формально рекуррентная нейронная сеть представляет собой параметризированную функцию RNN, которая рекурсивно отображает входной вектор и скрытое состояние в новое скрытое состояние.

В момент времени \(t\) скрытое состояние ("память" сети) обновляется с помощью ввода \(v_t: h_t = RNN(h_t-1,v_t;\theta\)), где \(h_0\) - начальное состояние, а \(v_t\) - вход на временном шаге \(t\). 

В этой модели новая сетка объектов \(\widetilde{V}\) создаётся, при запуске RNN по каждой строке этого ввода. Рекурсивно для всех строк \(h \in {(1, ..., H'})\) и столбцов \(w \in {(1, ..., W'})\), новые объекты определяются как  \(\widetilde{V}_{h, w} = RNN(\widetilde{V}_{h, w-1}, \widetilde{V}_{h, w})\). Для того, чтобы захватить информацию о последовательном порядке в вертикальном направлении, используется обучаемое начальное скрытое состояние \(\widetilde{V}_{h, 0}\) для каждой строки.

Вектор \(h_t\) используется для суммирования истории декодирования: \(h_t = RNN(h_{t-1}, [y_{t-1};o_{t-1}])\)[5].

\section{Описание процесса обучения и оценки точности модели}

Сам процесс обучения состоит из 11 итераций(заданий), для каждой из которых введены свои критерии обучения. К примеру, для первой задаются такие параметры:
\begin{enumerate}
\item Learning rate = 0.001 - Скорость чтения.
\item Number of epochs = 100 - Количество эпох обучения.
\item Batch size = 20 - Размер батча.
\item ENC\_DROPOUT = 0.5 - Параметр "метода исключения". Используется для предотвращения переобучения нейронной сети.
\item DEC\_DROPOUT= 0.5
\item DownsampleImage = False - Параметр уменьшения размера хранилища данных.
\item Height of image = 60 - Высота изображения.
\item Width of image = 360 - Ширина изображения.
\end{enumerate}

Так как это самая первая итерация, тут нейронная сеть обучается 100 эпох на минимальном наборе изображений (1200 элементов). Но всё дальнейшее обучение будет проходить на большом "hugedataset" наборе изображений (104 000 элементов).

Единственным результатом обучения нейронной сети, который мы можем увидеть является проверочная часть (\ref{fig:image1.4}) в конце каждой эпохи и выглядит она так:

\;

\begin{figure}[H]
\center{\includegraphics[width=13cm]{it1.jpg}}
\center{\includegraphics[width=13cm]{it2.jpg}}
\caption{Вид нейронной сети в работе над проверочной частью.}
\label{fig:image1.4}
\end{figure}

\;

Это результат проверочной работы нейросети после 100-ой эпохи обучения на наборе изображений "generatedset".

После прохождения одной полной итерации, мы получаем оценку работы нейронной сети, в частности расчёт её ошибки и BLEU (Bi-Lingual Evaluation Understudy) значение, которое показывает насколько распознанная нейронной сетью формула совпадает с правильной формулой. 

\;

Validate average loss: 8.536643981933594

BLEU score = 12.42

\;

По мере прохождения обучения параметры нейронной сети будут изменяться следующим образом. После первой итерации будет изменён пакет данных с "generatedset" на "hugedataset". Далее для третьей итерации будет изменено значение "Learning rate":

\;

Learning rate = 0.01

\;

Для четвёртой итерации снова будет изменён параметр "Learning rate":

\;

Learning rate = 0.0001

\;

Для пятой обновлено значение "Batch size":

\;

Batch size = 16

\;

Перед запуском шестой итерации изменяем значение "DownsampleImage" c "False" на "True", оно останется в этом положении до окончания обучения.

Для запуска седьмой, поменяем сразу три параметра:

\;

\begin{enumerate}
\item Learning rate = 0.00001
\item ENC\_DROPOUT = 0.1
\item DEC\_DROPOUT = 0.1
\end{enumerate}

\;

Для восьмой выставим исходное значение переменных "DropOut":

\;

\begin{enumerate}
\item ENC\_DROPOUT = 0.5
\item DEC\_DROPOUT = 0.5
\end{enumerate}

\;

Для девятой сделаем такие изменения и переведём изображения в новый формат:

\;

\begin{enumerate}
\item Learning rate = 0.0001
\item Batch size = 6
\item Height of image = 128
\item Width of image = 512
\end{enumerate}

\;

Для десятой внесём такие значения:

\;

\begin{enumerate}
\item Learning rate = 0.00001
\item ENC\_DROPOUT = 0.1
\item DEC\_DROPOUT = 0.1
\end{enumerate}

\;

И, для завершающей обучение, одиннадцатой итерации изменяем эти параметры:

\;

\begin{enumerate}
\item Batch size = 3
\item ENC\_DROPOUT = 0.0
\item DEC\_DROPOUT = 0.0
\end{enumerate}


\section{Результаты численных экспериментов}

Итоги первой итерации: 

Validate average loss: 8.536643981933594

BLEU score = 12.42

\;

Итоги второй итерации: 

Validate average loss: 2.291751463846753

BLEU score = 23.42

\;

Итоги третьей итерации: 

Validate average loss: 3.773274381954685

BLEU score = 2.87

\;

Итоги четвёртой итерации: 

Validate average loss: 4.963756845783645

BLEU score = 10.19

\;

Итоги пятой итерации:  

Validate average loss: 4.795395610563912

BLEU score = 14.93

\;

Итоги шестой итерации: 

Validate average loss: 8.862708091735844

BLEU score = 16.16

\;

Итоги седьмой итерации:

Validate average loss: 4.913547956497258

BLEU score = 14.04

\;

Итоги восьмой итерации: 

Validate average loss: 5.368844568714796

BLEU score = 9.18

\;

Итоги девятой итерации: 

Validate average loss: 5.017368428675382

BLEU score = 5.70

\;

Итоги десятой итерации: 

Validate average loss: 5.047398713594534

BLEU score = 8.56

\;

Итоги одиннадцатой итерации:

Validate average loss: 3.983458673284735

BLEU score = 11.46

\;

\begin{figure}[H]
\center{\includegraphics[width=8cm]{bleugru.jpg}}
\caption{Пример графика значений BLEU score.}
\label{fig:image1.5}
\includegraphics[width=8cm]{lbgru.jpg}
\caption{Пример графика значений ошибки по батчам.}
\label{fig:image1.6}
\includegraphics[width=8cm]{valgru.jpg}
\caption{Пример графика значений ошибок при работе с проверочной частью.}
\label{fig:image1.7}
\end{figure}

Анализируя полученные результаты, стоит отметить, что при смене пакета изображений на "hugedataset", мы наблюдаем сильное улучшение результатов. Но, для первой и второй итераций параметры были неизменны. После прохождения третьей итерации можно заметить сильный спад результатов обучения, что и связанно с началом изменения критерий обучения. Пятая, шестая и седьмая итерации, по результатам обучения, примерно одинаковы. Для девятой итерации был введён новый формат изображений из-за чего качество распознавания нейронной сетью сильно снижается, но в конечном итоге мы получаем, судя по нашим результатам, среднее значение для BLEU score и Validate average loss.

\chapter{Распознавания формул на изображениях глубокой нейронной сетью, основанной на трансформере}

\section{Описание модели}

\begin{figure}[H]
\center{\includegraphics[width=12cm]{st.jpg}}\caption{Схема одного из первых примеров применения механизмов внимания в глубоких нейронных сетях при помощи машин больцмана терьего порядка, созданая Юго Ларошелем и Хинтоном[6].}
\label{fig:image2.1}
\end{figure}

\begin{figure}[H]
\center{\includegraphics[width=15cm]{tran.jpg}}\caption{Схема сети основанной на ''Трансформере'' , которая использовалась в обучении.}
\label{fig:image2.2}
\end{figure}


Нейронная сеть с моделью "Трансформер" (\ref{fig:image2.1}) так же как и первая модель является свёрточной. Отличие её в том, что для отпимизации её скорости и обучения , она оснащена "механизмом внимания". Вместо того, чтобы полученная в процессе обучения информация переходила из одного слоя в другой, используется механизм, который принимает решение какой элемент входной последовательности имеет важное значение для конкретной формулы выходной последовательности. 

\section{Описание процесса обучения и оценки точности модели}

Сам процесс обучения состоит из 11 итераций(заданий), для каждой из которых введены свои критерии обучения. К примеру для первой задаются такие параметры:
\begin{enumerate}
\item Learning rate = 0.001
\item Number of epochs = 100
\item Batch size = 20
\item ENC\_DROPOUT = 0.5
\item DEC\_DROPOUT= 0.5
\item DownsampleImage = False
\item Height of image = 60
\item Width of image = 360
\end{enumerate}

Так как это самая первая итерация, тут нейронная сеть обучается 100 эпох на минимальном наборе изображений (1200 элементов). Но всё дальнейшее обучение будет проходить на большом "hugedataset" наборе изображений (104 000 элементов).

После прохождения одной полной итерации, мы получаем оценку работы нейронной сети, в частности расчёт её ошибки и BLEU (Bi-Lingual Evaluation Understudy) значение, которое показывает насколько распознанная нейронной сетью формула совпадает с правильной формулой. 

\;

Validate average loss: 0.4935080707032117

BLEU score = 77.71

\;

По мере прохождения обучения параметры нейронной сети будут изменяться следующим образом. После первой итерации будет изменён пакет данных с "generatedset" на "hugedataset". Далее для третьей итерации будет изменено значение "Learning rate":

\;

Learning rate = 0.01

\;

Для четвёртой итерации снова будет изменён параметр "Learning rate":

\;

Learning rate = 0.0001

\;

Для пятой обновлено значение "Batch size":

\;

Batch size = 16

\;

Перед запуском шестой итерации изменяем значение "DownsampleImage" c "False" на "True", оно останется в этом положении до окончания обучения.

Для запуска седьмой, поменяем сразу три параметра:

\;

\begin{enumerate}
\item Learning rate = 0.00001
\item ENC\_DROPOUT = 0.1
\item DEC\_DROPOUT = 0.1
\end{enumerate}

\;

Для восьмой выставим исходное значение переменных "DropOut":

\;

\begin{enumerate}
\item ENC\_DROPOUT = 0.5
\item DEC\_DROPOUT = 0.5
\end{enumerate}

\;

Для девятой сделаем такие изменения и переведём изображения в новый формат:

\;

\begin{enumerate}
\item Learning rate = 0.0001
\item Batch size = 6
\item Height of image = 128
\item Width of image = 512
\end{enumerate}

\;

Для десятой внесём такие значения:

\;

\begin{enumerate}
\item Learning rate = 0.00001
\item ENC\_DROPOUT = 0.1
\item DEC\_DROPOUT = 0.1
\end{enumerate}

\;

И, для завершающей обучение, одиннадцатой итерации изменяем эти параметры:

\;

\begin{enumerate}
\item Batch size = 3
\item ENC\_DROPOUT = 0.0
\item DEC\_DROPOUT = 0.0
\end{enumerate}

\section{Результаты численных экспериментов}

Итоги первой итерации: 

Validate average loss: 0.4935080707032117

BLEU score = 77.71

\;

Итоги второй итерации: 

Validate average loss: 0.17141498625278473

BLEU score = 93.28

\;

Итоги третьей итерации:

Validate average loss: 2.70721435546875

BLEU score = 17.54

\;

\;

Итоги четвёртой итерации: 

Validate average loss: 2.249237537384033

BLEU score = 25.63

\;

Итоги пятой итерации:

Validate average loss: 2.248100757598877

BLEU score = 26.14

\;

Итоги шестой итерации: 

Validate average loss: 2.2580454349517822

BLEU score = 24.55

\;

Итоги седьмой итерации:

Validate average loss: 2.2548861503601074

BLEU score = 24.29

\;

Итоги восьмой итерации: 

Validate average loss: 2.4864206314086914

BLEU score = 16.67

\;

Итоги девятой итерации:

Validate average loss: 2.5360090732574463

BLEU score = 14.42

\;

Итоги десятой итерации: 

Validate average loss: 2.2908096313476562

BLEU score = 21.89

\;

Итоги одиннадцатой итерации:

Validate average loss: 2.1443867683410645

BLEU score = 25.05

\;

\begin{figure}[H]
\center{\includegraphics[width=8cm]{bleutr.jpg}}
\caption{Пример графика значений BLEU score.}
\label{fig:image2.3}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=8cm]{lbtr.jpg}}
\caption{Пример графика значений ошибки по батчам.}
\label{fig:image2.4}
\end{figure}
\begin{figure}[H]
\center{\includegraphics[width=8cm]{valtr.jpg}}
\caption{Пример графика значений ошибок при работе с проверочной частью.}
\label{fig:image2.5}
\end{figure}

Как можно заметить, значение ошибки и BLEU score после первых двух итераций сильно возросло, связано это с тем, что параметры обучения не изменялись, лишь был произведён переход с одного пакета данных на другой. Чего нельзя сказать после прохождения нейронной сетью третьей итерации, для прохождения которой был изменён параметр "Learning rate". Для четвёртой, пятой,  шестой и седьмой мы получили примерно одинаковые значения, что уже говорит о более-менее стабильной работе нейронной сети при смене некоторых параметров. Перед седьмой итерацией мы изменили значение "DropOut", что никак не повлияло, а когда вернули это значение в исходное положение перед восьмой итерацией, результаты сильно понизились. Для девятой так же наблюдается снижение результатов из-за нового формата изображений, но по итогу, в десятой и одиннадцатой мы наблюдаем улучшение результатов.

\clearpage
\addcontentsline{toc}{section}{Заключение}
\section*{Заключение}

В данной работе обучены две нейронных сети, одна из которых основана на GRU ячейке, а вторая на модели "Трансформер". Были получены две полностью рабочие нейронные сети, способные распознавать формулы любой сложности с изображений. По результатам обучения можно отметить, что нейронная сеть на модели "Трансформер" справилась с поставленной задачей намного лучше, чем нейронная сеть, основанная на GRU ячейке, даже с учётом того, что обе нейронные сети не всегда имели высшие показатели результатов в ходе обучения. Так же стоит отметить то, что сеть с GRU ячейкой, в отличии от второй сети, требовала в несколько раз больше времени и ресурсов для обучения, что не всегда является возможным для "домашнего" обучения. 



\indent




% ----------------------------------------------------------------
\bibliographystyle{gost705}
%%\bibliography{Eskin_ENF_I_Bibl}

\begin{thebibliography}{99}
\itemsep=0ex

\bibitem{Culloch1943} {W.S. McCulloch, W. Pitts.} {A logical calculus of the ideas immanent in nervous activity}~// {The Bulletin of Mathematical Biophysics}.
\newblock 1943. 

\bibitem{Widrow1964} {Bernard Widrow.} {Pattern Recognition and Adaptive Control}~// {IEEE Transaction on Applications and Industry}.
\newblock 1964. 

\bibitem{Geoffrey2011} {Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis.} {Two distributed-state models for generating high-dimensional time series}~// {Journal of Machine Learning Research}.
\newblock 2011.

\bibitem{Yuntian} {Yuntian Deng, Sasha Rush, Hyliu.} {Neural model for converting Image-to-Markup}~// {by Yuntian Deng zenodo.org/record/56198}.
\newblock 2016.

\bibitem{Yuntian5} {Yuntian Deng, Anssi Kanervisto, Alexander M. Rush.} {A Visual Markup Decompiler}~// {What You Get Is What You See}.
\newblock 16 Sep 2016.

\bibitem{Larochelle} {Larochelle H., Hinton G. E.} {Learning to Combine Foveal Glmpses with a Third-Order Boltzmann Machine}~// {Advances in Neural Information Processind System 23}.
\newblock 2010.

%\bibitem{Basser1991} {Basser\,P.~J., Roth\,B.~J.} {Stimulation of a myelinated nerve axon by
%  electromagnetic induction}~// {Medical \& Biological Engineering \&
%  Computing}.
%\newblock 1991.
%\newblock V.\,29, No.\,3.
%\newblock P.\,261--268.

%\bibitem{Abaya2012} {Abaya\,T V~F, Blair\,S, Tathireddy\,P, Rieth\,L, Solzbacher\,F}. {A 3D
%  glass optrode array for optical neural stimulation.}~// {Biomedical
%  optics express}.
%\newblock 2012.
%\newblock V.\,3, No.\,12.
%\newblock P.\,3087--3104.
%
%\bibitem{Abaya2012a} {Abaya\,T V~F, Diwekar\,M, Blair\,S, Tathireddy\,P, Rieth\,L,
%  Clark\,G~a, Solzbacher\,F}. {Characterization of a 3D optrode array for
%  infrared neural stimulation.}~// {Biomedical optics express}.
%\newblock 2012.
%\newblock V.\,3, No.\,9.
%\newblock P.\,2200--19.
%
%\bibitem{Zhang2009} {Zhang\,Jiayi, Laiwalla\,Farah, Kim\,Jennifer~a, Urabe\,Hayato, {Van
%  Wagenen}\,Rick, Song\,Yoon-Kyu, Connors\,Barry~W, Zhang\,Feng,
%  Deisseroth\,Karl, Nurmikko\,Arto~V}. {Integrated device for optical
%  stimulation and spatiotemporal electrical recording of neural activity in
%  light-sensitized brain tissue.}~// {Journal of neural engineering}.
%\newblock 2009.
%\newblock V.\,6, No.\,5.
%\newblock 055007\,p.
%
%\bibitem{Kravitz2011} {Kravitz\,Alexxai~V, Kreitzer\,Anatol~C}. {Optogenetic manipulation of
%  neural circuitry in vivo.}~// {Current opinion in neurobiology}.
%\newblock 2011.
%\newblock V.\,21, No.\,3.
%\newblock P.\,433--9.
%
%\bibitem{Ozden2013} {Ozden\,Ilker, Wang\,Jing, Lu\,Yao, May\,Travis, Lee\,Joonhee,
%  Goo\,Werapong, O'Shea\,Daniel~J, Kalanithi\,Paul, Diester\,Ilka,
%  Diagne\,Mohamed, Deisseroth\,Karl, Shenoy\,Krishna~V, Nurmikko\,Arto~V}. {A
%  coaxial optrode as multifunction write-read probe for optogenetic studies in
%  non-human primates.}~// {Journal of neuroscience methods}.
%\newblock 2013.
%\newblock V. 219, No.\,1.
%\newblock P.\,142--54.
%
%\bibitem{Abaya2014} {Abaya\,Tanya V~F, Diwekar\,Mohit, Blair\,Steve, Tathireddy\,Prashant,
%  Rieth\,Loren, Solzbacher\,Florian}. {Deep-tissue light delivery via optrode
%  arrays.}~// {Journal of biomedical optics}.
%\newblock 2014.
%\newblock V.\,19, No.\,1.
%\newblock 15006\,p.
%
%\bibitem{Peterson2014} {Peterson\,E~J, Tyler\,D~J}. {Motor neuron activation in peripheral
%  nerves using infrared neural stimulation.}~// {Journal of neural
%  engineering}.
%\newblock 2014.
%\newblock V.\,11, No.\,1.
%\newblock 016001\,p.
%
%\bibitem{Shapiro2012} {Shapiro\,Mikhail~G, Homma\,Kazuaki, Villarreal\,Sebastian,
%  Richter\,Claus-Peter, Bezanilla\,Francisco}. {Infrared light excites cells by
%  changing their electrical capacitance.}~// {Nature communications}.
%\newblock 2012.
%\newblock V.\,3.
%\newblock P.\,736--1--736--10.
%
%\bibitem{Poher2008} {Poher\,V, Grossman\,N, Kennedy\,G~T, Nikolic\,K, Zhang\,H~X, Gong\,Z,
%  Drakakis\,E~M, Gu\,E, Dawson\,M~D, French\,P M~W, Degenaar\,P, Neil\,M a~a}.
%  {Micro-LED arrays: a tool for two-dimensional neuron stimulation}~//
%  {Journal of Physics D: Applied Physics}.
%\newblock 2008.
%\newblock V.\,41, No.\,9.
%\newblock P.\,094014--1--094014--1--9.
%
%\bibitem{Siegel1999} {Basic Neurochemistry: Molecular, Cellular, and Medical Aspects},
%  /\,Ed.\,G.G..\,Siegel, B.W..\,Agranoff, R.W..\,Albers, S.K..\,Fisher,
%  M.D..\,Uhler.
%\newblock New York: Lippincott-Raven Publishers, 1999.
%\newblock 1184\,p.
%
%\bibitem{Segelstein1981} {Segelstein\,D.~J.} {The complex refractive index of water}: M.sc.
%  thesis~/ University of Missouri.
%\newblock 1981.
%
%\bibitem{Min2009} {Min\,Younjin, Kristiansen\,Kai, Boggs\,Joan~M, Husted\,Cynthia,
%  Zasadzinski\,Joseph~a, Israelachvili\,Jacob}. {Interaction forces and
%  adhesion of supported myelin lipid bilayers modulated by myelin basic
%  protein.}~// {Proceedings of the National Academy of Sciences of the
%  United States of America}.
%\newblock 2009.
%\newblock V. 106, No.\,9.
%\newblock P.\,3154--3159.
%
%\bibitem{Kuznetsov1962} {Д.С.\,Кузнецов}. {Специальные функции},
%  /\,Ed.\,Высшая.\,школа.
%\newblock Высшая школа: Москва, 1962.
%\newblock 249\,p.
%
%\bibitem{Abramowitz1972} {Abramowitz\,Milton, Stegun\,Irene~A.} {Handbook of Mathematical
%  Functions With Formulas, Graphs, and Mathematical Tables}.
%\newblock Washington, DC: NBS Applied Mathematics Series 55, National Bureau of
%  Standards, 1972.
%\newblock 1045\,p.
%
%\bibitem{Yasumoto2006} {Yasumoto\,K.} Electromagnetic Theory and Applications for Photonic Crystals.
%\newblock Taylor \& Francis, Boca Raton, 2006.

\end{thebibliography}

\newpage

\addcontentsline{toc}{section}{Техника безопасности при работе с компьютером}
\section*{Техника безопасности при работе с компьютером}
Компьютерная техника в настоящее время используется практически во всей деятельности человека. Применение данного оборудования способно оказывать негативное влияние на здоровье и даже привести к чрезвычайным ситуациям и несчастным случаям. Для того, чтобы этого избежать, необходимо соблюдать технику безопасности при работе с компьютером.\\
Рассмотрим правила безопасности по каждому этапу работы с компьютером.\\
1. До начала работы: проверить исправность электропроводки, розеток и вилок компьютера, заземление ПК.\\
2. Во время работы:\\
- необходимо аккуратно обращаться с проводами;\\
- запрещается работать с неисправным компьютером;\\
- нельзя заниматься очисткой компьютера, когда он находится под напряжением;\\
- недопустимо самостоятельно проводить ремонт оборудования при отсутствии специальных навыков;\\
- нельзя располагать рядом с компьютером жидкости, а также работать с мокрыми руками;\\
- нельзя в процессе работы с ПК прикасаться к другим металлическим конструкциям (например, батареям);\\
- не допускается курение и употребление пищи в непосредственной близости с ПК.\\
3. В аварийных ситуациях:\\
- при любых неполадках необходимо сразу отсоединить ПК от сети;\\
- в случае обнаружения оголенного провода исключить контакт с ним;\\
- в случае возникновения пожара принять меры по его тушению с использованием огнетушителей;\\
- в случае поражения человека током оказать первую помощь и вызвать скорую медицинскую помощь.\\
4. По окончании работы:\\
- выключить компьютер;\\
- желательно провести влажную уборку рабочего места;\\
- отключить электропитание.\\
Для исключения всевозможных рисков, нужно ответственно относиться ко всем этапам использования компьютера. Пользователь может и должен контролировать весь цикл взаимодействия с техникой. Процесс соблюдения всех этих несложных правил должен быть непрерывным и комплексным.




% ----------------------------------------------------------------

\end{document} 
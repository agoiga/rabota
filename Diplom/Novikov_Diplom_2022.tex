\documentclass[bachelor,subf
%,href
%,fixint=false
%,facsimile
%,times
%,draft
]{disser}
%%\documentclass[12pt,a4paper]{report}


\usepackage[
  a4paper, mag=1000, includefoot,
  left=2.5cm, right=1cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm,
]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\ifpdf\usepackage{epstopdf}\fi

\usepackage[pdftex,unicode,colorlinks,filecolor=blue,linkcolor=black,citecolor=blue,bookmarksopen,
pdfhighlight=/N]{hyperref}

%\onehalfspacing
%\chaptername{Гла}
\sloppy



%\def\thefootnote{\arabic{footnote})}
\def\ts{\thinspace}
\def\a{\alpha}
\def\b{\beta}
\def\d{\delta}
\def\D{\Delta}
\def\e{\varepsilon}
\def\f{\varphi}
\def\g{\gamma}
\def\G{\Gamma}
\def\o{\omega}
\def\O{\Omega}
\def\s{\sigma}
\def\vs{\varsigma}
\def\p{\partial}
\def\pr{\prime}
\def\vt{\vartheta}
\def\tl{\tilde}
\def\wtl{\widetilde}
\def\what{\widehat}
\def\l{\lambda}
\def\L{\Lambda}
\def\wtl{\widetilde}
\def\il{\int\limits}
\def\sul{\sum\limits}
\def\Rl{\rm Re}
\def\dsp{\displaystyle}
\def\ol{\overline}
\def\oe{\kappa}
\def\e{\varepsilon}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\def\t{\tilde}
\def\ln{\mathop{\mathrm{ln}\, }}
\def\ds{\displaystyle}
\def\dif{{\rm d}}
\def\what{\widehat}
\def\oLH{\omega_{\scriptscriptstyle \rm LH}}
\def\OLH{\Omega_{\scriptscriptstyle \rm LH}}
\def\oH{\omega_{\scriptscriptstyle \rm H}}
\def\OH{\Omega_{\scriptscriptstyle \rm H}}
\def\oUH{\omega_{\scriptscriptstyle \rm UH}}
\def\Op{\Omega_{\scriptscriptstyle \rm p}}
\def\op{\omega_{\scriptscriptstyle \rm p}}
\def\nue{\nu_{\scriptstyle \rm e}}
\def\nuei{\nu_{\scriptstyle \rm ei}}
\def\nuen{\nu_{\scriptstyle \rm en}}
\def\nuin{\nu_{\scriptstyle \rm in}}
\def\p{\scriptscriptstyle \rm p}

\def\vte{{\textit{v}}_{\scriptscriptstyle {T}_{\rm e}}}


\def\cP{\tilde P}
\def\cH{cB}
\def\EB{\bf E}
\def\Bb{\bf B}
\def\cHB{c{\bf B}}

\def\E{{\cal E}\kern-6.8pt{\cal E}\kern-6.8pt{\cal E}}
%\def\H{{\cal H}\kern-8.9pt{\cal H}\kern-9.1pt{\cal H}}
\def\H{{\cal H}\kern-9.9pt{\cal H}\kern-9.9pt{\cal H}}

\def\ve{{\boldmath {\mbox{$\varepsilon$}}}}

\def\r{\rho\kern-6.2pt\rho\kern-6.3pt\rho}
\def\ph{\phi\kern-6.7pt\phi\kern-6.7pt\phi}

\def\fo{\varphi\kern-8.2pt\varphi\kern-8.3pt\varphi}
\def\ol{\overline}

\def\i{\textrm{i}}
\def\d{\textrm{d}}

\renewcommand{\vec}[1]{{\bf#1}}
\renewcommand{\phi}{\varphi}
\renewcommand{\partial}{\d}

\renewcommand{\baselinestretch}{1.25}


\begin{document}
% Название организации
\institution{~\\ МИНИСТЕРСТВО НАУКИ И ВЫСШЕГО ОБРАЗОВАНИЯ РОССИЙСКОЙ ФЕДЕРАЦИИ\\ ~\\ Федеральное государственное автономное образовательное учреждение высшего образования\\ «Нижегородский государственный университет
им. Н.\,И.\,Лобачевского» \\ ~\\
Радиофизический факультет \\
Кафедра электродинамики \\ ~\\
Направление «Радиофизика»}
% Имя лица, допускающего к защите (зав. кафедрой)

\title{ВЫПУСКНАЯ  КВАЛИФИКАЦИОННАЯ  РАБОТА}
% Тема
\topic{Глубокие нейронные сети для распознования формул на изображениях}

% Автор
\author{Новиков Е.\,А.} % ФИО
\group{0418ДБФ9Г} % Группа

% Научный руководитель
\sa      {Еськин В.\,А.}
\sastatus{к.~ф.-м.~н.}



% Город и год
\city{Нижний Новгород}
\date{\number\year}
\maketitle

% ----------------------------------------------------------------

% ----------------------------------------------------------------

% для замены в именах авторов использую регулярное выражение from (?<name>[A-Z?-?])\.\s(?<sname>[A-Z?-?])\. to  {${name}.\,${sname}.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\renewcommand{\chaptername}{}
%\renewcommand{\bibname}{Литература}
%\renewcommand{\figurename}{Рисунок}

%\setcounter{page}{6}
%\maketitle
% ----------------------------------------------------------------
\tableofcontents
% ----------------------------------------------------------------

\addcontentsline{toc}{section}{Введение}
\section*{Введение}
На данном этапе развития технологий в современном мире всё чаще появляется необходимость автоматизированного чтения текста с изображений, фото или видео. Под текстом может подразумеваться абсолютно всё: рукописный текст, формулы, требующие переноски из старых учебников и книг в сеть или быстрый перевод текста с бумажных носителей. В любом случае, такие технологии сильно облегают жизнь людей.

Сейчас такую возможность нам предоставляют нейронные сети, которые, по сути, имитируют некоторые аспекты умственной деятельности человека, так как нейронная сеть – это модель, математически созданная на основе биологических нейронных сетей и их функционирования. Первая попытка создания нейронной сети принадлежит Уоррену Мак-Каллоку и Уолтеру Питтсу, которые формируют понятие нейронной сети [1]. А через несколько лет Дональд Хебб предлагает первый алгоритм обучения.

Интерес к нейронным сетям обусловлен их успешным применением в самых разных областях – медицина, бизнес, геология, физика. Их практикуют везде, где нужно находить решения задач управления, классификации или прогнозирования. Так Бернард Уидроу и его студент Хофф создали Адалин, использовавшийся для задач предсказания и адаптивного управления [2]. В 2007 году Джеффри Хитоном в университете Торонто созданы алгоритмы глубокого обучения многослойных нейронных сетей. Для этого была использована ограниченная машина Больцмана[3]. Для обучения должно использоваться большое количество образов, которые могут быть распознанны. После обучения на выходе имеется быстро работающая программа с возможность. решения конкретных задач.

Целью работы является обучение нейросети, основанной на GRU ячейке и нейросети, основанной на модели "Трансформер", с последующим сравнением их результатов.

Актуальность данной работы состоит в сравнении двух нейронных сетей, разработанных на двух различных моделях. Данное решение принято исходя из того, чтоб подсчитать, с каким успехом развиваются нейронные сети и с какой скоростью они будут обучаться, имея одинаковый набор данных.

Данная работа состоит из двух глав. В первой главе рассматривается эксперимент с распознаванием формул на изображениях глубокой нейронной сетью, основанной на GRU ячейке. Во второй исследуется распознавания формул на изображениях глубокой нейронной сетью, основанной на модели "Трансформер".

\indent





\chapter{Распознавания формул на изображениях глубокой нейронной сетью, основанной на GRU ячейке}
%\addcontentsline{toc}{chapter}{Глава I Рассеяние плоской электромагнитной волны на миелинизированном аксоне}
\section{Описание набора данных и подготовки набора данных}

В качестве набора данных используются изображения с формулами, взятых с гарвардского проекта[4]. Набор состоит из двух пакетов "generateset" и ''hugedataset''. Первый представляет собой изображения с формулами в количестве 1700 штук, второй 104000 изображений."generatedset" будет использоваться только для первой итерации каждой нейронной сети, для последующих итераций будет использован ''hugedataset''.  Изначально мы имеем изображения формул на листе A4.

\includegraphics[width=10cm]{G:/Diplom/h1.jpg}
\includegraphics[width=4cm]{G:/Diplom/h11.jpg}

Для оптимизации работы нейронной сети, предварительно, эти изображения обрезаются. 

\includegraphics[width=10cm]{G:/Diplom/h2.jpg}
\includegraphics[width=4cm]{G:/Diplom/h22.jpg}

Создаётся отдельный файл, где каждая из формул прописана в печатном виде и имеет свой номер. После чего, формулы нормализуются. Для наибольшего успеха обучения, из пакета данных формулы исключаются те, что имеют большое количество токенов и грамматические ошибки.

\section{Описание модели}

\includegraphics[width=5cm]{G:/Diplom/om.png}

Данная сеть представляет собой свёрточную нейронную сеть с несколькими стандартными нейронными компонентами из области зрения и обработки естественного языка. Сначала он извлекает объекты изображений с помощью свёрточной сети (CNN) и упорядочивает объекты в сетке, затем каждая строка кодируется с помощью рекурентной сети (RNN). После используется декодер с механизмом внимания. Визуальные признаки изображений извлекаются с помощью многослойной нейронной сети.Формально рекуррентная нейронная сеть представляет собой параметризированную функцию RNN, которая рекурсивно отображает входной вектор и скрытое состояние в новое скрытое состояние.

В момент времени \(t\) скрытое состояние обновляется с помощью ввода \(v_t: h_t = RNN(h_t-1,v_t;\theta\)), где \(h_0\) - начальное состояние. 

В этой модели новая сетка объектов \(\widetilde{V}\) создаётся, при запуске RNN по каждой строке этого ввода. Рекурсивно для всех строк \(h \in {(1, ..., H'})\) и столбцов \(w \in {(1, ..., W'})\), новые объекты определяются как  \(\widetilde{V}_{h, w} = RNN(\widetilde{V}_{h, w-1}, \widetilde{V}_{h, w})\). Для того, чтобы захватить информацию о последовательном порядке в вертикальном направлении, используется обучаемое начальное скрытое состояние \(\widetilde{V}_{h, 0}\) для каждой строки.

Далее декодером генерируются марки разметки \({y_t}\) на основе последовательности сетки аннотаций \(\widetilde{V}\). Вектор \(h_t\) используется для суммирования истории декодирования: \(h_t = RNN(h_{t-1}, [y_{t-1};o_{t-1}])\)[5].

\section{Описание процесса обучения и оценки точности модели}

В самом начале нужно подготовить среду обучения нейронной сети. Для этого устанавливаем соответствующие плагины:

\includegraphics[width=5cm]{G:/Diplom/pl.jpg}

Сам процесс обучения состоит из 11 итераций, для каждой из которых введены свои критерии обучения. К примеру для первой задаются такие параметры:

\includegraphics[width=11cm]{G:/Diplom/t1.jpg}

Так как это самая первая итерация, тут нейронная сеть обучается 100 эпох на минимальном наборе изображений (1200 элементов). Но всё дальнейшее обучение будет проходить на большом "hugedataset" наборе изображений (104 000 элементов).

Единственным результатом обучения нейронной сети, который мы можем увидеть является проверочная часть в конце каждой эпохи и выглядит она так:

\includegraphics[width=14cm]{G:/Diplom/it1.jpg}

\includegraphics[width=14cm]{G:/Diplom/it2.jpg}

Это результат проверочной работы нейросети после 100-ой эпохи обучения на наборе изображений "generatedset".

\includegraphics[width=7cm]{G:/Diplom/rt1.1.jpg}

Так же, после прохождения одной полной итерации, мы получаем оценку работы нейронной сети, в частности расчёт её ошибки и BLEU (Bi-Lingual Evaluation Understudy) значение, которое показывает насколько распознанная нейронной сетю формула совпадает с правильной фомулой. 

По мере прохождения обучения параметры нейронной сети будут изменяться следующим образом. После первой итерации будет изменён пакет данных с "generatedset" на "hugedataset". Далее для третьей итерации будет изменено значение "Learning rate":

\includegraphics[width=5cm]{G:/Diplom/lr1.jpg}

Для четвёртой итерации снова будет изменён параметр "Learning rate":

\includegraphics[width=5cm]{G:/Diplom/lr2.jpg}

Для пятой обновлено значение "Batch size":

\includegraphics[width=5cm]{G:/Diplom/bs1.jpg}

Перед запуском шестой итерации изменяем значение "DownsampleImage" c "False" на "True", оно останется в этом положении до окончания обучения:

\includegraphics[width=5cm]{G:/Diplom/dsi1.jpg}

Для запуска седьмой, поменяем сразу три параметра:

\includegraphics[width=5cm]{G:/Diplom/lr3.jpg}  \; \includegraphics[width=5cm]{G:/Diplom/do1.jpg}

Для восьмой выставим исходное значение переменных "DropOut":

\includegraphics[width=5cm]{G:/Diplom/do2.jpg}

Для девятой сделаем такие изменения и переведём изображения в новый формат:

\includegraphics[width=5cm]{G:/Diplom/lr4.jpg} \; \includegraphics[width=5cm]{G:/Diplom/bs2.jpg} \; \includegraphics[width=5cm]{G:/Diplom/f1.jpg}

Для десятой вносём такие значения:

\includegraphics[width=5cm]{G:/Diplom/lr5.jpg} \; \includegraphics[width=5cm]{G:/Diplom/do3.jpg}

И, для завершающей обучение, одиннадцатой итерации изменяем эти параметры:

\includegraphics[width=5cm]{G:/Diplom/lr5.jpg} \; \includegraphics[width=5cm]{G:/Diplom/bs3.jpg} \; \includegraphics[width=5cm]{G:/Diplom/do4.jpg}

\section{Результаты численных экспериментов}

Итоги первой итерации: \qquad\qquad\qquad\qquad Итоги второй итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt1.1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt2.1.jpg}

Итоги третьей итерации: \qquad\qquad\qquad\quad\; Итоги четвёртой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt3.1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt4.1.jpg}

Итоги пятой итерации: \qquad\qquad\qquad\qquad\; Итоги шестой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt5.1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt6.1.jpg}

Итоги седьмой итерации: \qquad\qquad\qquad\quad\; Итоги восьмой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt7.1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt8.1.jpg}

Итоги девятой итерации: \qquad\qquad\qquad\quad\; Итоги десятой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt9.1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt10.1.jpg}

Итоги одиннадцатой итерации:

\includegraphics[width=7cm]{G:/Diplom/rt11.1.jpg}

Анализируя полученные результаты, стоит отметить, что при смене пакета изображений на "hugedataset", мы наблюдаем сильное улучшение результатов. Но, для первой и второй итераций параметры были неизменны. После прохождения 3 итерации можно заметить сильный спад результатов обучения, что и связанно с началом изменения критерий обучения. Пятая, шестая и седьмая итерации, по результатам обучения, примерно одинаковы. Для девятой итерации был введён новый формат изображений из-за чего качество распознования нейронной сетью сильно снижается, но в конечном итоге мы получаем, судя по нашим результатам, среднее значение для BLEU score и Validate average loss.

\chapter{Распознавания формул на изображениях глубокой нейронной сетью, основанной на трансформере}

\section{Описание модели}

Нейронная сеть с моделью "Трансформер" так же как и первая модель является свёрточной и состоит из слоёв. Отличие её в том, что для отпимизации её скорости и обучения , она оснащена "механизмом внимания". Вместо того, чтобы полученная в процессе обучения информация переходила из одного слоя в другой, используется механизм, который принимает решение какой элемент входной последовательности имеет важное значение для конкретной формулы выходной последовательности. 

\section{Описание процесса обучения и оценки точности модели}

Сам процесс обучения так же состоит из 11 итераций, для каждой из которых введены свои критерии обучения. К примеру для первой задаются те же параметры:

\includegraphics[width=12cm]{G:/Diplom/t1.jpg}

Так как это самая первая итерация, тут нейронная сеть обучается 100 эпох на минимальном наборе изображений (1200 элементов). Но всё дальнейшее обучение будет проходить на большом "hugedataset" наборе изображений (104 000 элементов). Так же, после прохождения одной полной итерации, мы получаем оценку работы нейронной сети, в частности расчёт её ошибки и BLEU (Bi-Lingual Evaluation Understudy) значение, которое показывает насколько распознанная нейронной сетю формула совпадает с правильно написанной. 

По мере прохождения обучения параметры нейронной сети будут изменяться таким же образом. После первой итерации будет изменён пакет данных с "generatedset" на "hugedataset". Далее для третьей итерации будет изменено значение "Learning rate":

\includegraphics[width=5cm]{G:/Diplom/lr1.jpg}

Для четвёртой итерации снова будет изменён параметр "Learning rate":

\includegraphics[width=5cm]{G:/Diplom/lr2.jpg}

Для пятой обновлено значение "Batch size":

\includegraphics[width=5cm]{G:/Diplom/bs1.jpg}

Перед запуском шестой итерации изменяем значение "DownsampleImage" c "False" на "True", оно останется в этом положении до окончания обучения:

\includegraphics[width=5cm]{G:/Diplom/dsi1.jpg}

Для запуска седьмой, поменяем сразу три параметра:

\includegraphics[width=5cm]{G:/Diplom/lr3.jpg}  \; \includegraphics[width=5cm]{G:/Diplom/do1.jpg}

Для восьмой выставим исходное значение переменных "DropOut":

\includegraphics[width=5cm]{G:/Diplom/do2.jpg}

Для девятой сделаем такие изменения и переведём изображения в новый формат:

\includegraphics[width=5cm]{G:/Diplom/lr4.jpg} \; \includegraphics[width=5cm]{G:/Diplom/bs2.jpg} \; \includegraphics[width=5cm]{G:/Diplom/f1.jpg}

Для десятой вносём такие значения:

\includegraphics[width=5cm]{G:/Diplom/lr5.jpg} \; \includegraphics[width=5cm]{G:/Diplom/do3.jpg}

И, для завершающей обучение, одиннадцатой итерации изменяем эти параметры:

\includegraphics[width=5cm]{G:/Diplom/lr5.jpg} \; \includegraphics[width=5cm]{G:/Diplom/bs3.jpg} \; \includegraphics[width=5cm]{G:/Diplom/do4.jpg}

\section{Результаты численных экспериментов}

Итоги первой итерации: \qquad\qquad\qquad\qquad Итоги второй итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt1.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt2.jpg}

Итоги третьей итерации: \qquad\qquad\qquad\quad\; Итоги четвёртой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt3.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt4.jpg}

Итоги пятой итерации: \qquad\qquad\qquad\qquad\; Итоги шестой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt5.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt6.jpg}

\;

\;

Итоги седьмой итерации: \qquad\qquad\qquad\quad\; Итоги восьмой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt7.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt8.jpg}

Итоги девятой итерации: \qquad\qquad\qquad\quad\; Итоги десятой итерации: 

\includegraphics[width=7cm]{G:/Diplom/rt9.jpg} \qquad\qquad\; \includegraphics[width=7cm]{G:/Diplom/rt10.jpg}

Итоги одиннадцатой итерации:

\includegraphics[width=7cm]{G:/Diplom/rt11.jpg}

Как можно заметить, значение ошибки и BLEU score после первых двух итераций сильно возрасло, связано это с тем, что параметры обучения не изменялись, лишь был произведён переход с одного пакета данных на другой. Чего нельзя сказать после прохождения нейронной сетью 3 итерации, для прохождения которой был изменён параметр "Learning rate". Для четвёртой, пятой,  шестой и седьмой мы получили примерно одинаковые значения, что уже говорит о более менее стабильной работе нейронной сети при смене некоторых параметров. Перед седьмой итерацией мы изменили значение "DropOut", что никак не повлияло, а когда вернули это значение в исходное положение перед восьмой итерацией, результаты сильно понизились. Для девятой так же наблюдается снижение результатов из-за нового формата изображений, но по итогу, в десятой и одиннадцатой мы наблюдаем улучшение результатов.

\clearpage
\addcontentsline{toc}{section}{Заключение}
\section*{Заключение}

В данной работе обучены две нейронных сети, одна из которых основана на GRU ячейке, а вторая на модели "Трансформер". Были получены две полностью рабочие нейронные сети, способные распозновать формулы любой сложности с изображений. По результатам обучения можно отметить, что нейронная сеть на модели "Трансформер" справилась с поставленной задачей намного лучше, чем нейронная сеть, основанная на GRU ячейке, даже с учётом того, что обе нейронные сети не всегда имели высшие показатели результатов в ходе обучения. Так же стоит отметить то, что сеть с GRU ячейкой, в отличии от второй сети, требовала в несколько раз больше времени и ресурсов для обучения, что не всегда является возможным для "домашнего" обучения. 



\indent




% ----------------------------------------------------------------
\bibliographystyle{gost705}
%%\bibliography{Eskin_ENF_I_Bibl}

\begin{thebibliography}{99}
\itemsep=0ex

\bibitem{Culloch1943} {W.S. McCulloch, W. Pitts.} {A logical calculus of the ideas immanent in nervous activity}~// {The Bulletin of Mathematical Biophysics}.
\newblock 1943. 

\bibitem{Widrow1964} {Bernard Widrow.} {Pattern Recognition and Adaptive Control}~// {IEEE Transaction on Applications and Industry}.
\newblock 1964. 

\bibitem{Geoffrey2011} {Graham W. Taylor, Geoffrey E. Hinton, Sam T. Roweis.} {Two distributed-state models for generating high-dimensional time series}~// {Journal of Machine Learning Research}.
\newblock 2011.

\bibitem{Yuntian} {Yuntian Deng, Sasha Rush, Hyliu.} {Neural model for converting Image-to-Markup}~// {by Yuntian Deng yuntiandeng.com}.
\newblock 2016.

\bibitem{Yuntian5} {Yuntian Deng, Anssi Kanervisto, Alexander M. Rush.} {A Visual Markup Decompiler}~// {What You Get Is What You See}.
\newblock 16 Sep 2016.

%\bibitem{Basser1991} {Basser\,P.~J., Roth\,B.~J.} {Stimulation of a myelinated nerve axon by
%  electromagnetic induction}~// {Medical \& Biological Engineering \&
%  Computing}.
%\newblock 1991.
%\newblock V.\,29, No.\,3.
%\newblock P.\,261--268.

%\bibitem{Abaya2012} {Abaya\,T V~F, Blair\,S, Tathireddy\,P, Rieth\,L, Solzbacher\,F}. {A 3D
%  glass optrode array for optical neural stimulation.}~// {Biomedical
%  optics express}.
%\newblock 2012.
%\newblock V.\,3, No.\,12.
%\newblock P.\,3087--3104.
%
%\bibitem{Abaya2012a} {Abaya\,T V~F, Diwekar\,M, Blair\,S, Tathireddy\,P, Rieth\,L,
%  Clark\,G~a, Solzbacher\,F}. {Characterization of a 3D optrode array for
%  infrared neural stimulation.}~// {Biomedical optics express}.
%\newblock 2012.
%\newblock V.\,3, No.\,9.
%\newblock P.\,2200--19.
%
%\bibitem{Zhang2009} {Zhang\,Jiayi, Laiwalla\,Farah, Kim\,Jennifer~a, Urabe\,Hayato, {Van
%  Wagenen}\,Rick, Song\,Yoon-Kyu, Connors\,Barry~W, Zhang\,Feng,
%  Deisseroth\,Karl, Nurmikko\,Arto~V}. {Integrated device for optical
%  stimulation and spatiotemporal electrical recording of neural activity in
%  light-sensitized brain tissue.}~// {Journal of neural engineering}.
%\newblock 2009.
%\newblock V.\,6, No.\,5.
%\newblock 055007\,p.
%
%\bibitem{Kravitz2011} {Kravitz\,Alexxai~V, Kreitzer\,Anatol~C}. {Optogenetic manipulation of
%  neural circuitry in vivo.}~// {Current opinion in neurobiology}.
%\newblock 2011.
%\newblock V.\,21, No.\,3.
%\newblock P.\,433--9.
%
%\bibitem{Ozden2013} {Ozden\,Ilker, Wang\,Jing, Lu\,Yao, May\,Travis, Lee\,Joonhee,
%  Goo\,Werapong, O'Shea\,Daniel~J, Kalanithi\,Paul, Diester\,Ilka,
%  Diagne\,Mohamed, Deisseroth\,Karl, Shenoy\,Krishna~V, Nurmikko\,Arto~V}. {A
%  coaxial optrode as multifunction write-read probe for optogenetic studies in
%  non-human primates.}~// {Journal of neuroscience methods}.
%\newblock 2013.
%\newblock V. 219, No.\,1.
%\newblock P.\,142--54.
%
%\bibitem{Abaya2014} {Abaya\,Tanya V~F, Diwekar\,Mohit, Blair\,Steve, Tathireddy\,Prashant,
%  Rieth\,Loren, Solzbacher\,Florian}. {Deep-tissue light delivery via optrode
%  arrays.}~// {Journal of biomedical optics}.
%\newblock 2014.
%\newblock V.\,19, No.\,1.
%\newblock 15006\,p.
%
%\bibitem{Peterson2014} {Peterson\,E~J, Tyler\,D~J}. {Motor neuron activation in peripheral
%  nerves using infrared neural stimulation.}~// {Journal of neural
%  engineering}.
%\newblock 2014.
%\newblock V.\,11, No.\,1.
%\newblock 016001\,p.
%
%\bibitem{Shapiro2012} {Shapiro\,Mikhail~G, Homma\,Kazuaki, Villarreal\,Sebastian,
%  Richter\,Claus-Peter, Bezanilla\,Francisco}. {Infrared light excites cells by
%  changing their electrical capacitance.}~// {Nature communications}.
%\newblock 2012.
%\newblock V.\,3.
%\newblock P.\,736--1--736--10.
%
%\bibitem{Poher2008} {Poher\,V, Grossman\,N, Kennedy\,G~T, Nikolic\,K, Zhang\,H~X, Gong\,Z,
%  Drakakis\,E~M, Gu\,E, Dawson\,M~D, French\,P M~W, Degenaar\,P, Neil\,M a~a}.
%  {Micro-LED arrays: a tool for two-dimensional neuron stimulation}~//
%  {Journal of Physics D: Applied Physics}.
%\newblock 2008.
%\newblock V.\,41, No.\,9.
%\newblock P.\,094014--1--094014--1--9.
%
%\bibitem{Siegel1999} {Basic Neurochemistry: Molecular, Cellular, and Medical Aspects},
%  /\,Ed.\,G.G..\,Siegel, B.W..\,Agranoff, R.W..\,Albers, S.K..\,Fisher,
%  M.D..\,Uhler.
%\newblock New York: Lippincott-Raven Publishers, 1999.
%\newblock 1184\,p.
%
%\bibitem{Segelstein1981} {Segelstein\,D.~J.} {The complex refractive index of water}: M.sc.
%  thesis~/ University of Missouri.
%\newblock 1981.
%
%\bibitem{Min2009} {Min\,Younjin, Kristiansen\,Kai, Boggs\,Joan~M, Husted\,Cynthia,
%  Zasadzinski\,Joseph~a, Israelachvili\,Jacob}. {Interaction forces and
%  adhesion of supported myelin lipid bilayers modulated by myelin basic
%  protein.}~// {Proceedings of the National Academy of Sciences of the
%  United States of America}.
%\newblock 2009.
%\newblock V. 106, No.\,9.
%\newblock P.\,3154--3159.
%
%\bibitem{Kuznetsov1962} {Д.С.\,Кузнецов}. {Специальные функции},
%  /\,Ed.\,Высшая.\,школа.
%\newblock Высшая школа: Москва, 1962.
%\newblock 249\,p.
%
%\bibitem{Abramowitz1972} {Abramowitz\,Milton, Stegun\,Irene~A.} {Handbook of Mathematical
%  Functions With Formulas, Graphs, and Mathematical Tables}.
%\newblock Washington, DC: NBS Applied Mathematics Series 55, National Bureau of
%  Standards, 1972.
%\newblock 1045\,p.
%
%\bibitem{Yasumoto2006} {Yasumoto\,K.} Electromagnetic Theory and Applications for Photonic Crystals.
%\newblock Taylor \& Francis, Boca Raton, 2006.

\end{thebibliography}





% ----------------------------------------------------------------

\end{document} 